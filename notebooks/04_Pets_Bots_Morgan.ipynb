{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "04_Pets_Bots_Morgan.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cocteau/computing2021/blob/main/notebooks/04_Pets_Bots_Morgan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGMI5o7TtJ-M"
      },
      "source": [
        "Pets, Bots and Morgan Fairchild\n",
        "---------------\n",
        "\n",
        "<img src=\"http://prod-upp-image-read.ft.com/a1d70e02-f763-11e7-a4c9-bbdefa4f210b\" width=500>\n",
        "\n",
        "<pre>A distressed fox in a school in Peckham \n",
        "A squirrel that had to be coaxed out of a drainpipe in Hendon \n",
        "A swan trapped in an electrical substation in Greenwich \n",
        "A lovebird trapped inside a lift in an office block in Paddington.\n",
        "</pre>\n",
        "\n",
        "###Review\n",
        "\n",
        "\n",
        "###Rescue\n",
        "\n",
        "We are going to take a deeper look into pandas, the Python Data Analysis Library. It contains an extremely powerful object called a DataFrame (a term borrowed from R).\n",
        "\n",
        "A DataFrame is designed to work with tabular data. Think about a Python implementation of a spreadsheet. Each row of the data frame refers to a different \"unit of observation\" and each column refers to a particular measurement or attribute of each unit. We have seen examples of this, but here are more (I borrowed them from the [\"Data is Plural\" website](https://www.data-is-plural.com/).\n",
        "\n",
        "In each case, the tabular data can be represented as a CSV. You will recall that CSV stands for Comma Separated Values. We'll show you the underlying CSV in each example.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIEhkPzs7JLp"
      },
      "source": [
        "\n",
        "1. The Tour de France makes data on its riders available on its [website](https://www.letour.fr/en/history), but in a pretty fancy format. It has been scraped and republished [here](https://github.com/camminady/LeTourDataSet) as [a CSV](https://github.com/camminady/LeTourDataSet/blob/master/Riders.csv). Each row is a rider in one of the Tours, starting from 1903. The columns record data about each rider like their average pace during the race.\n",
        "<pre>\n",
        "Year,Rider,Rank,Time,DistanceKilometer,PersonalAvgPace,Hours,Minutes,Seconds,Team,RiderNumber,TotalSeconds,Gap,Bonus,Points,NumberStages,ListedAvgPace\n",
        "1903,MAURICE GARIN,1,94h 33' 14'',2428.0,25.678,94.0,33.0,14.0,TDF 1903 ***,1,340394.0,-,,,6.0,25.679000000000002\n",
        "1903,LUCIEN POTHIER,2,97h 32' 35'',2428.0,24.892,97.0,32.0,35.0,TDF 1903 ***,37,351155.0,+ 02h 59' 21'',,,6.0,25.679000000000002\n",
        "1903,FERNAND AUGEREAU,3,99h 02' 38'',2428.0,24.514,99.0,2.0,38.0,TDF 1903 ***,39,356558.0,+ 04h 29' 24'',,,6.0,25.679000000000002\n",
        "1903,RODOLPHE MULLER,4,99h 12' 44'',2428.0,24.473000000000003,99.0,12.0,44.0,TDF 1903 ***,33,357164.0,+ 04h 39' 30'',,,6.0,25.679000000000002\n",
        "1903,JEAN-BAPTISTE FISCHER,5,99h 41' 58'',2428.0,24.353,99.0,41.0,58.0,TDF 1903 ***,12,358918.0,+ 05h 08' 44'',,,6.0,25.679000000000002\n",
        "</pre>\n",
        "There's even [a notebook](https://github.com/camminady/LeTourDataSet/blob/master/analysis.ipynb) to help walk you through the analysis!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b0Ghcnw7rGR"
      },
      "source": [
        "from pandas import read_csv\n",
        "\n",
        "tour = read_csv(\"https://raw.githubusercontent.com/camminady/LeTourDataSet/master/Riders.csv\",index_col=0)\n",
        "tour.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5ji1kood_3S"
      },
      "source": [
        "If we want a more spreadsheet like display, we can use a Colab extension. Let's try it out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdTBri8jdUGA"
      },
      "source": [
        "%load_ext google.colab.data_table\n",
        "tour"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1lMqhHe-GAH"
      },
      "source": [
        "\n",
        "2. TSA checkpoint travel numbers. You can see the data [here](https://www.tsa.gov/coronavirus/passenger-throughput). Each row is a day, and the columns are TSA checkpoint counts for each of three years on a given day. Here is what the CSV might look like. \n",
        "<pre>\n",
        "Date,2021 Traveler Throughput,2020 Traveler Throughput,2019 Traveler Throughput\n",
        "6/2/2021,1587910.0,304436,2370152\n",
        "6/1/2021,1682752.0,267742,2247421\n",
        "5/31/2021,1900170.0,353261,2499002\n",
        "5/30/2021,1650454.0,352947,2555578\n",
        "5/29/2021,1605810.0,268867,2117180\n",
        "</pre>\n",
        "We can run this through [Mr. Data Converter](https://shancarter.github.io/mr-data-converter/) to see other ways to organize this data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P82eF-Qp3go8"
      },
      "source": [
        "#@title\n",
        "# Since the TSA data is in a table, we should be able to just\n",
        "# read it into a DataFrame using read_html(). However, the URL \n",
        "# specifies the protocol https and read_html() only takes URLs\n",
        "# with the protocols http, ftp, and file protocols. So, we \n",
        "# instead make a request using the get() function, then parse\n",
        "# out the table using BeautifulSoup and then finally use \n",
        "# read_html() on the table. Whew.\n",
        "\n",
        "from requests import get\n",
        "from bs4 import BeautifulSoup\n",
        "from pandas import read_html\n",
        "\n",
        "# Fetch the page\n",
        "response = get(\"https://www.tsa.gov/coronavirus/passenger-throughput\")\n",
        "\n",
        "# parse the HTML\n",
        "bs = BeautifulSoup(response.text)\n",
        "\n",
        "# use find() to get the \"table\" tag\n",
        "table = bs.find(\"table\")\n",
        "\n",
        "# then read it as a pandas dataframe\n",
        "df = read_html(str(table))[0]\n",
        "\n",
        "# and show the first 5 entries\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAcUZrMJ-bE1"
      },
      "source": [
        "3. A group called the [Endangered Language Alliance](https://elalliance.org/) has created a map of languages spoken in NYC and counties nearby. There are 700 of them!  Their [downloadable data set](https://languagemap.nyc/Data) is a table, where each row refers to a language n a NYC region. So among the second row you see Abruzzese in Astoria. Columns include the number of speakers of the language globally and the size of the speaking population in the NYC area.\n",
        "<pre>\n",
        "Language,Endonym,World Region,Country,Global Speakers,Language Family,Location,Size,Status\n",
        "Abaza,Абаза,Western Asia,\"Turkey,Russia\",\"49,800\",Abkhaz-Adyge,Wayne (NJ),Smallest,Residential\n",
        "Abruzzese,,Southern Europe,Italy,,Indo-European,Astoria,Small,Residential\n",
        "Abruzzese,,Southern Europe,Italy,,Indo-European,Little Italy,Small,Historical\n",
        "Acehnese,Bahsa Acèh,Southeastern Asia,Indonesia,\"3,500,000\",Austronesian,Astoria,Smallest,Community\n",
        "Acehnese,Bahsa Acèh,Southeastern Asia,Indonesia,\"3,500,000\",Austronesian,Elmhurst,Smallest,Residential\n",
        "</pre>\n",
        "The data are [downloadable as a CSV](\"https://raw.githubusercontent.com/cocteau/computing2021/main/data/nyc-language-data.csv\"). The cell below shows you how to upload data to Colab from your computer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feo0TJOq7TLq"
      },
      "source": [
        "from google.colab import files\n",
        "from io import BytesIO\n",
        "\n",
        "# this will prompt you to pick a file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# this will read that file into a pandas dataframe called \"lang\"\n",
        "lang = read_csv(BytesIO(uploaded['nyc-language-data.csv']))\n",
        "\n",
        "# have a look\n",
        "lang.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbX6HTUMdvZV"
      },
      "source": [
        "We've been looking at the fancy Colab display of a DataFrame. Let's switch it off for the moment and go back to the regular representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abfh0aIndwR3"
      },
      "source": [
        "%unload_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHm9Qsyp7eBB"
      },
      "source": [
        "In each case, we've gone from some kind of tabular input data (and HTML `<table>` or a CSV) and parsed the entries in the rows and columns to make a DataFrame. We'll now look at what that object affords us in terms of computation. \n",
        "\n",
        "Before we start, there is plenty of documentation on pandas -- there are great task-based help entries from [the official pandas website](https://pandas.pydata.org/docs/) and there's even a great [cheatsheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf).\n",
        "\n",
        "<img src=\"https://github.com/cocteau/computing2021/raw/main/images/Screen%20Shot%202021-06-03%20at%208.11.31%20AM.png\" width=500>\n",
        "\n",
        "As our running example today, we'll use a data set that comes from the London Fire Brigade. From [their website](https://data.london.gov.uk/dataset/animal-rescue-incidents-attended-by-lfb)...\n",
        "\n",
        ">The London Fire Brigade attends a range of non-fire incidents (which we call 'special services'). These 'special services' include assistance to animals that may be trapped or in distress.\n",
        "<br><br>\n",
        "We routinely get asked for information about the number of such incident attended by the London Fire Brigade and this data is published on the London Datastore to assist those who require it.\n",
        "<br><br>\n",
        "The data is provided from January 2009 and isupdated monthly. A range of information is supplied for each incident including some location information (postcode, borough, ward), as well as the data/time of the incidents. We do not routinely record data about animal deaths or injuries.\n",
        "<br><br>\n",
        "Please note that any cost included is a notional cost calculated based on the length of time rounded up to the nearest hour spent by Pump, Aerial and FRU appliances at the incident and charged at the current Brigade hourly rate.\n",
        "\n",
        "<img src=\"https://github.com/cocteau/computing2021/raw/main/images/Screen%20Shot%202021-06-03%20at%207.47.38%20AM.png\" width=500>\n",
        "\n",
        "The CSV from the fire brigade contains one row per incident, and the columns describe aspects of what happened, the brigade resources involved, a time and a location. The the URL associated with the \"Download\" link points directly to the CSV file. Let's copy it and then use it as an argument to `read_csv()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KlYFtUIOBpR"
      },
      "source": [
        "from pandas import read_csv\n",
        "\n",
        "lfb = read_csv(\"https://data.london.gov.uk/download/animal-rescue-incidents-attended-by-lfb/8a7d91c2-9aec-4bde-937a-3998f4717cd8/Animal%20Rescue%20incidents%20attended%20by%20LFB%20from%20Jan%202009.csv\")\n",
        "lfb.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGQ6Xo7kP7Pl"
      },
      "source": [
        "So `.head()` is a method of a pandas DataFrame that takes as a possible argument the number of rows you'd like to see. One of the attributes of this object is a list of column names stored in `.columns`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GsBqr_MeR2O"
      },
      "source": [
        "lfb.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmKODD3weVWc"
      },
      "source": [
        "The fancy printing has a default maximum column count of 20 and our `lfb` DataFrame has 31. We can get around it if we call the display directly (and there *has* to be another work around, but for the moment, this is the best I could come up with). The name of the object that is being used for the fancy display is a `DataTable`. I hesitated even introducing the code below because it will be confusing. But for the moment, the payoff in terms of the brigade data is huge. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5FCSH4mP6-v"
      },
      "source": [
        "from google.colab.data_table import DataTable\n",
        "\n",
        "# Use the DataTable display with just 10 rows in each page of the display and\n",
        "# a max columns of 40 (as our lfb data set has 31 < 40)\n",
        "\n",
        "DataTable(lfb, num_rows_per_page=10,max_columns=40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6RdQcMhP5HQ"
      },
      "source": [
        "Look at the data and consider what stories we might find here and the kinds of computation we need to perform. Work in pairs and write out options. Use the sorting feature of the fancy Colab table display to your advantage!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMsDR37EOTNn"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8964JSdgW_U"
      },
      "source": [
        "The pandas DataFrame object has a lot of built-in data analysis functionality. Much of it has to do with sorting, subsetting and summarizing the data in one form or another. For example, we can summarize the `AnimalGroupParent` and see which kinds of animals are most often identified as needing assistance.\n",
        "\n",
        "To do that, we first want to isolate the `AnimalGroupParent` column. That's a subsetting operation -- focus just on the one column. We use square brackets (as you might expect) to select columns  by a single name..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdYHeNmGhyvn"
      },
      "source": [
        "lfb[\"AnimalGroupParent\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKWToAYyh_N6"
      },
      "source": [
        "... or using `.head()` to see more than just the first and last five entries..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBM1OeXVh3hc"
      },
      "source": [
        "lfb[\"AnimalGroupParent\"].head(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSt_r1yhiG61"
      },
      "source": [
        "... or `.tail()` for a collection of the last entries..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dcs-KF7ZiGBe"
      },
      "source": [
        "lfb[\"AnimalGroupParent\"].tail(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHKmt8jwiWO0"
      },
      "source": [
        "In technical terms, this single column is called a Series. That's just how pandas refers to one column.\n",
        "\n",
        "Finally let's count each category and presenting a summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxc3jnkBiW1S"
      },
      "source": [
        "lfb[\"AnimalGroupParent\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNb6oYBbi1_V"
      },
      "source": [
        "The lesson here is not to own a cat? The lower portion of the list is interesting as well. A Hedgehog? A Bull? \n",
        "\n",
        "You can also create the summary more directly using `lfb` and just passing a column name to `.value_counts()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exC1yIW8sBlG"
      },
      "source": [
        "lfb.value_counts(\"AnimalGroupParent\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I-UsTUZsEAp"
      },
      "source": [
        "This kind of simple counting works well as a summary because we have a qualitative variable. It probably wouldn't be so effective with quantitative data that will presumably have a large number of unique values. More on these distinctions when we start making graphics. \n",
        "\n",
        "Now your turn. Pick a couple other columns and create a summary. Perhaps something you came up with as a group."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND85fpfWkGzg"
      },
      "source": [
        "# your code here\n",
        "\n",
        "lfb[\"PropertyType\"].value_counts().head(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbplKaTatS-g"
      },
      "source": [
        "There are other summaries we can perform on a column -- all the basic summary statistics are available to you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIKxSHi-tTmk"
      },
      "source": [
        "lfb['IncidentNotionalCost(£)'].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmeCx4qpter2"
      },
      "source": [
        "lfb['IncidentNotionalCost(£)'].median()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucquMBZZteX5"
      },
      "source": [
        "lfb['IncidentNotionalCost(£)'].sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPsH2pUvkJ7n"
      },
      "source": [
        "You can create subsets of more than two columns by using a list of column names. So we would use\n",
        "<pre>\n",
        "[\"AnimalGroupParent\",\"CalYear\",\"Borough\"]\n",
        "</pre>\n",
        "if we wanted just the two columns `\"AnimalGroupParent\"`, `\"CalYear\"` and `\"Borough\"`. Let's have a look. I've added space between the outer square brackets specifying subsetting and the inner square brackets defining the list of column names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwRFtndNlCVl"
      },
      "source": [
        "lfb[ [\"AnimalGroupParent\",\"CalYear\",\"Borough\"]  ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_ahduCYl0jC"
      },
      "source": [
        "You can give the new, reduced DataFrame a new name, too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s-Cv77Pl1ME"
      },
      "source": [
        "lfb_small = lfb[[\"AnimalGroupParent\",\"CalYear\",\"Borough\"]]\n",
        "\n",
        "lfb_small.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBTPyRaZmto6"
      },
      "source": [
        "The attribute `.shape` in a DataFrame holds the number of rows and columns. By comparison..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuH4eOQ_ms01"
      },
      "source": [
        "lfb.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyB0ZS1OnIBg"
      },
      "source": [
        "Suppose we wanted to compare animals by year. How many cats in 2020 versus 2019, say? One common technique for subsetting the rows of a DataFrame is with a logical mask. \n",
        "\n",
        "We have not talked about an important built-in type, the boolean. We've seen integers and floats and strings. Booleans take on just one of two values `True` or `False`. They are handy for instructing the computer to do various things if some condition is true or not. \n",
        "\n",
        "For subsetting, we might want to keep only the rows for which a column entry takes a certain value. Here we want to test if each animal is a dog or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDCr43ppoCOv"
      },
      "source": [
        "lfb[\"CalYear\"] > 2020"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPhijn4XoV9r"
      },
      "source": [
        "This returns a column of Booleans (again, a Series), one for each row in the data set. If you compare this output to the first and last five rows of `lfb` you hopefully see things matching up.\n",
        "\n",
        "If you want to find out how many dogs we have, we can use the function `sum()` on this new column. It will turn the `True`'s to a 1 and the `False`'s to 0. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG_5rpAto2Es"
      },
      "source": [
        "sum(lfb[\"AnimalGroupParent\"] == \"Dog\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeV6dp8jpQQh"
      },
      "source": [
        "So that's 1187 dogs (which matches with our `.value_counts()` output. \n",
        "\n",
        "This boolean Series can also be used to subset our DataFrame, keeping only the rows for which our series has a `True`. Let's try it out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr1j5Yg1qVgR"
      },
      "source": [
        "lfb[   lfb[\"AnimalGroupParent\"] == \"Dog\"    ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohs5fBvxqlYG"
      },
      "source": [
        "Below we make a new DataFrame called `dogs` and then tally up the years. Use the cell below to try cats. Same pattern?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKNFWi0tqmQT"
      },
      "source": [
        "dogs = lfb[lfb[\"AnimalGroupParent\"] == \"Cat\"]\n",
        "\n",
        "# count the years that dogs were in need of assistance\n",
        "# we add the argument \"sort\" to tell pandas to print\n",
        "# things in year order and not sort the years by counts\n",
        "\n",
        "dogs.value_counts(\"CalYear\",sort=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QLLCYfdtG9h"
      },
      "source": [
        "Of course we can do this in one line as well. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clwkSc7atHiu"
      },
      "source": [
        "lfb[lfb[\"AnimalGroupParent\"] == \"Dog\"].value_counts(\"CalYear\",sort=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tgUC8YUtUrk"
      },
      "source": [
        "What does this show us? Maybe a graphic would help. For the moment we are going to use a graphing library called [Plotly Express](https://plotly.com/python/plotly-express/). It's a pretty easy way to make plots. First, let's use our `.value_counts()` and turn the output from a Series by moving the row index into a proper column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Uq8HDu8twGm"
      },
      "source": [
        "# create the counts\n",
        "years = lfb.value_counts(\"CalYear\").reset_index()\n",
        "\n",
        "# then move the \"index\" to a column that's called \"count\"\n",
        "years.rename(columns={0:\"bob\",\"CalYear\":\"Year\"},inplace=True)\n",
        "\n",
        "# and have a look\n",
        "years"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nESqGM74jp-"
      },
      "source": [
        "# now make th plot using this new data\n",
        "\n",
        "from plotly.express import bar\n",
        "\n",
        "bar(years,x=\"CalYear\",y=\"count\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA6t6TAvumaw"
      },
      "source": [
        "While we're at it, let's summarize one of the quantitative variables in the data set. The notional cost of the rescue. Here is a histogram..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcd27q1suBI4"
      },
      "source": [
        "from plotly.express import histogram\n",
        "\n",
        "histogram(lfb, x='IncidentNotionalCost(£)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD7IYeqTuv1d"
      },
      "source": [
        "... and here we use `sort_values()` to arrange the rows in the dataset from most expensive to least."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_vmggSbuKGG"
      },
      "source": [
        "lfb.sort_values(\"IncidentNotionalCost(£)\",ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBffCn2JudbZ"
      },
      "source": [
        "lfb.sort_values(\"IncidentNotionalCost(£)\",ascending=False).head(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpJU08766YVa"
      },
      "source": [
        "Some of the columns we have in our data set are text -- sometimes \"free text\" descriptions, sometimes text as names of places, say. Let's look at the `Ward`. What do you notice about the early and late names?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-TdupMQKA2f"
      },
      "source": [
        "lfb[\"Ward\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT5Mh3I2RuAl"
      },
      "source": [
        "If we summarize the column, we find the following. What do you notice?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBEkDPfOCXEN"
      },
      "source": [
        "lfb[\"Ward\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oocIBGpcR3kK"
      },
      "source": [
        "We can act on columns of strings (the `str` data type). We can access the methods available to a string object using the `.str` attribute of the Series. Here we take the `Ward` entries and make them uppercase. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9dBFTDoISm7"
      },
      "source": [
        "lfb[\"Ward\"].str.upper()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMKpH0w8SjIU"
      },
      "source": [
        "Chaining methods, we can add `.value_counts()` and see how the regularization to upper case changes things. What do you see?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYyLLrIaKGsh"
      },
      "source": [
        "lfb[\"Ward\"].str.upper().value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z64-REG8Y0Kf"
      },
      "source": [
        "Now let's look at the `FinalDescription` and see what they look like. The `sample()` method picks elements from a column or DataFrame at random. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5KQRw8HTr6N"
      },
      "source": [
        "lfb[\"FinalDescription\"].sample(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ly7EzNRrZvo6"
      },
      "source": [
        "Looking to text as data, we might want to think about the final descriptions as sentences -- that's not a perfect characterization, but something to work with. We are going to use [a package called `textblob`.](https://textblob.readthedocs.io/en/dev/) Let's load up some of the data it needs..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Itico8tUqrY"
      },
      "source": [
        "!python -m textblob.download_corpora"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHN-RyFTcOQn"
      },
      "source": [
        "... and now let's create a `TextBlob` object and see what it does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oI-z3LzUHT_"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "description = \"CAT TRAPPED BETWEEN CONSERVATORY AND FENCE\".lower()\n",
        "b = TextBlob(description)\n",
        "\n",
        "b.pos_tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crkY0FgMcuDy"
      },
      "source": [
        "In terms of analysis, this might suggest we search for verbs in each description and maybe see if there are certain animals more often \"trapped\" or \"stuck\" than others. Just a thought."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQTQNnN9tJ-q"
      },
      "source": [
        "\n",
        "If we want to have a look at a particular element in a list, we can \"chain\" our selections. We know that `[  ]` with a string returns a single column of the data frame with that name. The object is of type `Series` and when you subset using `[  ]` on a series, you can give rownames (typically numbers) and it will give you the items you want. The rules here are like the rules you used for lists and character strings - individual numbers or so-called slices separated by a colon `m:n`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2EhHMLctJ-q"
      },
      "source": [
        "lfb[\"Ward\"][10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naBMCEUztJ-q"
      },
      "source": [
        "lfb[\"FinalDescription\"][1500:1520]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0FaKnVjtJ-r"
      },
      "source": [
        "Now, this kind of \"chaining\" of the `[  ]` turns out to be a little fragile. It depends on the order you do things and to clean it up a bit, Pandas has added two methods that make the operation clearer. We'll focus on `.loc[  ]` and let you read about it's counterpart `.iloc[  ]` [here](https://medium.com/dunder-data/selecting-subsets-of-data-in-pandas-6fcd0170be9c).\n",
        "\n",
        "So `.loc[  ]` is again square brackets so we are subsetting. It allows us to pull out rows and columns at one time, separating the choices with a comma.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJEo-4o2tJ-r"
      },
      "source": [
        "lfb.loc[1500:1520,\"FinalDescription\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK1nlPsjtJ-r"
      },
      "source": [
        "lfb.loc[14:17,[\"Ward\",\"Borough\",\"PostcodeDistrict\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WX3uomTtJ-s"
      },
      "source": [
        "Now, to add a column, we can use the simple subsetting expression with  `[  ]`. We're just dealing with columns (not rows and columns at the same time). So, you can add a column to a DataFrame by just referring to it's name and assigniung it some value. The value can be a list or it can be a single value that will be repeated to fill the column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv50zQhBtJ-s"
      },
      "source": [
        "# take 417 values (the numbers 0 to 417) and add them as the last column\n",
        "\n",
        "lfb[\"UpperWard\"] = lfb[\"Ward\"].str.upper()\n",
        "lfb.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDJoOJgHtJ-s"
      },
      "source": [
        "# Repeat adding a single column, this time it will repeat the empty string 417 times\n",
        "\n",
        "lfb[\"notes\"] = \"\"\n",
        "lfb.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cT25_oltJ-s"
      },
      "source": [
        "But, if we want to update notes, say the `\"notes\"` column associated with the 1501th row, we need to use our friend `.loc[  ]` because we are indexing both rows and columns. So here's how we get the `\"FinalDescription\"` from that row..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x4r0dBCtJ-t"
      },
      "source": [
        "lfb.loc[1500,\"FinalDescription\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYCVsELltJ-t"
      },
      "source": [
        "... and here's how we would introduce a note."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCXda4EjtJ-t"
      },
      "source": [
        "lfb.loc[1500,\"notes\"] = \"RSPCA involved\"\n",
        "\n",
        "# we can see all the data for the 15th row, say by using .loc[] again...\n",
        "lfb.loc[1500,]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XRWHjKWtJ-u"
      },
      "source": [
        "The printing here is strange, I admit. If you asked for more than one row things get back to normal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CprXadYmtJ-u"
      },
      "source": [
        "lfb.loc[1500:1501,]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCjDc6v6qfxu"
      },
      "source": [
        "As a parting note, we notice that the data have location -- a latitude and longitude. This lets us place the incidents on a map! We are going to do a little work here to clean up the data -- it's some code, but you should be able to follow it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQJyaBvyfWW3"
      },
      "source": [
        "# turn all the descriptions into strings, because many are in fact numbers\n",
        "lfb[\"LowerDesc\"] = lfb[\"FinalDescription\"].apply(str).str.lower()\n",
        "\n",
        "# take a sample of 1000 incidents, but sampling from just those with not-missing\n",
        "# coordinates and for which the description is not \"redacted\"\n",
        "\n",
        "samp = lfb[~lfb[\"Latitude\"].isna() & (~lfb[\"LowerDesc\"].str.contains(\"redacted\"))].sample(100).reset_index()\n",
        "\n",
        "# create a list of lists -- [lat,long] for each row\n",
        "locations = samp[['Latitude', 'Longitude']]\n",
        "locationlist = locations.values.tolist()\n",
        "\n",
        "locationlist[7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI3C--ObrHTU"
      },
      "source": [
        "For mapping, we will use the library `folium`. I am just going to give the code and we will come back to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG0p0iqpkjww"
      },
      "source": [
        "import folium\n",
        "\n",
        "map = folium.Map(location=[51.5, -0.1], zoom_start=10,tiles='CartoDB positron')\n",
        "for point in range(0, len(locationlist)):\n",
        "    folium.Marker(locationlist[point], popup=samp['FinalDescription'][point]).add_to(map)\n",
        "map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmlcnOmVrwnF"
      },
      "source": [
        "**Bots**\n",
        "\n",
        "We are headed to think a bit about bots. [Synthetic Messenger](http://syntheticmessenger.labr.io/ ) is a project that seems very doable given what you are learning in this class. The project creates a botnet that artificially inflates the value of climate news. Everyday it searches the internet for news articles covering climate change. Then 100 bots visit each article and click on every ad they can find. (by Sam Lavigne and Tega Brain)\n",
        "\n",
        "If we have time, let's map out what we need to do to make this work? What steps should we follow?\n",
        "\n",
        "**Morgan Fairchild**\n",
        "\n",
        "We started this class looking at emails and in the last day or so, a large number of emails from Anthony Fauci have been released as part of [an article from Buzzfeed](https://www.buzzfeednews.com/article/nataliebettendorf/fauci-emails-covid-response). The emails themselves are graphics and the story closes with a link to the emails themselves. How might what we've been learning help us figure out what's in this trove of documents?\n",
        "\n"
      ]
    }
  ]
}